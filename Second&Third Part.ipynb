{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second & Third Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing\n",
    "In this part, the main goal is to seperate training and testing data, filter out useless information, and combine tweets with emotions as labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Observe data_identification.csv to seperate train and test identifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of tweet_id is 1867535. From len() we verify that we successfully seperate training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_identification.csv\n",
    "train_test_set = pd.read_csv(\"dm19-lab2-nthu/data_identification.csv\",\n",
    "                         header=None, names=[\"id\", \"identification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tweet_id</td>\n",
       "      <td>identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  identification\n",
       "0  tweet_id  identification\n",
       "1  0x28cc61            test\n",
       "2  0x29e452           train\n",
       "3  0x2b3819           train\n",
       "4  0x2db41f            test"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop the first title row\n",
    "train_test_set = train_test_set.drop([0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id identification\n",
       "1  0x28cc61           test\n",
       "2  0x29e452          train\n",
       "3  0x2b3819          train\n",
       "4  0x2db41f           test\n",
       "5  0x2a2acc          train"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids = train_test_set[train_test_set['identification']=='train'].filter(['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = train_test_set[train_test_set['identification']=='test'].filter(['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0x29e452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0x2b3819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0x2a2acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0x2a8830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0x20b21d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0x2452cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0x2d729d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0x2ab56d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0x1f3657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0x1fcc53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id\n",
       "2   0x29e452\n",
       "3   0x2b3819\n",
       "5   0x2a2acc\n",
       "6   0x2a8830\n",
       "7   0x20b21d\n",
       "8   0x2452cf\n",
       "9   0x2d729d\n",
       "10  0x2ab56d\n",
       "11  0x1f3657\n",
       "12  0x1fcc53"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1455563"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411972"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Observe emotion.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From len(), I found that emotion.csv only has train_ids since the number of datas in emotion_set is same as train_ids. Thus, It's no need to do more process on emotion.csv but just read and save as pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#emotion.csv\n",
    "emotion_set = pd.read_csv(\"dm19-lab2-nthu/emotion.csv\",\n",
    "                         header=None, names=[\"id\", \"emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop the first title row\n",
    "emotion_set = emotion_set.drop([0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0x34cd80</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0x33f099</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0x2ae7b7</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0x2408d4</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0x2b193b</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       emotion\n",
       "1   0x3140b1       sadness\n",
       "2   0x368b73       disgust\n",
       "3   0x296183  anticipation\n",
       "4   0x2bd6e1           joy\n",
       "5   0x2ee1dd  anticipation\n",
       "6   0x34cd80           joy\n",
       "7   0x33f099       sadness\n",
       "8   0x2ae7b7       sadness\n",
       "9   0x2408d4         trust\n",
       "10  0x2b193b       sadness"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1455563"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "emotion_set.to_pickle(\"train_df.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Observe tweets_DM.json, get texts and tweets_id from raw data, and combine training data with emotion labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tweets_DM.json includes both train and test data, I use train_ids above to divide the file into two parts. In addition to seperating the file, I also merge emotion_set with train_tweet_df for further works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_DM.json\n",
    "tweets_df = pd.read_json(\"dm19-lab2-nthu/tweets_DM.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'tweet_...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2de2...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index                                            _source  \\\n",
       "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2     232  hashtag_tweets  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
       "3     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "4     989  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
       "\n",
       "            _crawldate   _type  \n",
       "0  2015-05-23 11:42:47  tweets  \n",
       "1  2016-01-28 04:52:09  tweets  \n",
       "2  2017-12-25 04:39:20  tweets  \n",
       "3  2016-01-24 23:53:05  tweets  \n",
       "4  2016-01-08 17:18:59  tweets  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_source_df = tweets_df['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df['text'] = tweets_source_df.apply(lambda x: x['tweet']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df['id'] = tweets_df['_source'].apply(lambda x: x['tweet']['tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_tweets_df = tweets_df.filter(['text','id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        id\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20\n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350\n",
       "2  Confident of your obedience, I write to you, k...  0x28b412\n",
       "3                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  0x1cd5b0\n",
       "4  \"Trust is not the same as faith. A friend is s...  0x2de201"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tweets_df = filter_tweets_df[filter_tweets_df['id'].isin(train_ids['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets_df = filter_tweets_df[~filter_tweets_df['id'].isin(train_ids['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1455563"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411972"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        id\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20\n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350\n",
       "3                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  0x1cd5b0\n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c\n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>0x218443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>0x2939d5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>0x26289a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text        id\n",
       "2   Confident of your obedience, I write to you, k...  0x28b412\n",
       "4   \"Trust is not the same as faith. A friend is s...  0x2de201\n",
       "9   When do you have enough ? When are you satisfi...  0x218443\n",
       "30  God woke you up, now chase the day #GodsPlan #...  0x2939d5\n",
       "33  In these tough times, who do YOU turn to as yo...  0x26289a"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add emotion labels to train dataframe\n",
    "train_tweets_df = train_tweets_df.merge(emotion_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "train_tweets_df.to_pickle(\"train_tweets_df.pkl\") \n",
    "test_tweets_df.to_pickle(\"test_tweets_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature engineering and Model Building\n",
    "Since feature engineering is related to model building, I would like to introduce both them in once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Combined pre-train w2v model with scikit learn machine learning model\n",
    "Each tweet consists of many words, so I think it's a good start to create \"sentence vectors\" as features by w2v model. After finishing feature engineering, I build some models by traditional machine learning.\n",
    "\n",
    "#### (1) Load GoogleNews vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "\n",
    "## Note: this model is very huge, this will take some time ...\n",
    "model_path = \"GoogleNews/GoogleNews-vectors-negative300.bin.gz\"\n",
    "w2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "print('load ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tweets_df = pd.read_pickle(\"train_tweets_df.pkl\")\n",
    "test_tweets_df = pd.read_pickle(\"test_tweets_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Tokenize and use w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize and transform each sentence into one vector by summing and averaging all word vectors in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(sentence):\n",
    "    tokens = nltk.wordpunct_tokenize(sentence.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize texts using tokenize_text()\n",
    "train_tweets_df['unigrams'] = train_tweets_df['text'].apply(lambda x: tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize texts using tokenize_text()\n",
    "test_tweets_df['unigrams'] = test_tweets_df['text'].apply(lambda x: tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>unigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>[people, who, post, \", add, me, on, #, snapcha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>[@, brianklaas, as, we, see, ,, trump, is, dan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>[now, issa, is, stalking, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, &lt;, lh, &gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>[@, riskshow, @, thekevinallison, thx, for, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>[still, waiting, on, those, supplies, liscus, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        id  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350   \n",
       "3                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  0x1cd5b0   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8   \n",
       "\n",
       "                                            unigrams  \n",
       "0  [people, who, post, \", add, me, on, #, snapcha...  \n",
       "1  [@, brianklaas, as, we, see, ,, trump, is, dan...  \n",
       "3    [now, issa, is, stalking, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, <, lh, >]  \n",
       "5  [@, riskshow, @, thekevinallison, thx, for, th...  \n",
       "6  [still, waiting, on, those, supplies, liscus, ...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save w2v index2word to a set\n",
    "index2word_set = set(w2v_google_model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v(tokens):\n",
    "    num_of_word = 0\n",
    "    sentence_vec = np.zeros(300)\n",
    "    avg_vec = np.zeros(300)\n",
    "    for word in tokens:\n",
    "        if word not in index2word_set:\n",
    "            continue\n",
    "        else:\n",
    "            sentence_vec = np.add(sentence_vec,w2v_google_model[word])\n",
    "            num_of_word += 1\n",
    "        if(num_of_word>0):\n",
    "            length = np.linalg.norm(sentence_vec)\n",
    "            avg_vec = sentence_vec/length\n",
    "    return avg_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate unigrams to vector using w2v()\n",
    "train_tweets_df['vector'] = train_tweets_df['unigrams'].apply(lambda x: w2v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets_df['vector'] = test_tweets_df['unigrams'].apply(lambda x: w2v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>[people, who, post, \", add, me, on, #, snapcha...</td>\n",
       "      <td>[-0.04398071877257593, 0.005973617156422666, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>[@, brianklaas, as, we, see, ,, trump, is, dan...</td>\n",
       "      <td>[-0.07402511888368858, -0.027571952599088873, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>[now, issa, is, stalking, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, &lt;, lh, &gt;]</td>\n",
       "      <td>[-0.08825499574715533, 0.021097196412366435, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>[@, riskshow, @, thekevinallison, thx, for, th...</td>\n",
       "      <td>[-0.05885132318051363, -0.02492928390044968, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>[still, waiting, on, those, supplies, liscus, ...</td>\n",
       "      <td>[-0.01618322244071664, -0.02359793926091037, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        id  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350   \n",
       "3                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  0x1cd5b0   \n",
       "5  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c   \n",
       "6       Still waiting on those supplies Liscus. <LH>  0x2c91a8   \n",
       "\n",
       "                                            unigrams  \\\n",
       "0  [people, who, post, \", add, me, on, #, snapcha...   \n",
       "1  [@, brianklaas, as, we, see, ,, trump, is, dan...   \n",
       "3    [now, issa, is, stalking, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, <, lh, >]   \n",
       "5  [@, riskshow, @, thekevinallison, thx, for, th...   \n",
       "6  [still, waiting, on, those, supplies, liscus, ...   \n",
       "\n",
       "                                              vector  \n",
       "0  [-0.04398071877257593, 0.005973617156422666, 0...  \n",
       "1  [-0.07402511888368858, -0.027571952599088873, ...  \n",
       "3  [-0.08825499574715533, 0.021097196412366435, 0...  \n",
       "5  [-0.05885132318051363, -0.02492928390044968, 0...  \n",
       "6  [-0.01618322244071664, -0.02359793926091037, 0...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add emotion labels to train dataframe\n",
    "train_tweets_df = train_tweets_df.merge(emotion_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>vector</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>[people, who, post, \", add, me, on, #, snapcha...</td>\n",
       "      <td>[-0.04398071877257593, 0.005973617156422666, 0...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>[@, brianklaas, as, we, see, ,, trump, is, dan...</td>\n",
       "      <td>[-0.07402511888368858, -0.027571952599088873, ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>[now, issa, is, stalking, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, &lt;, lh, &gt;]</td>\n",
       "      <td>[-0.08825499574715533, 0.021097196412366435, 0...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>[@, riskshow, @, thekevinallison, thx, for, th...</td>\n",
       "      <td>[-0.05885132318051363, -0.02492928390044968, 0...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>[still, waiting, on, those, supplies, liscus, ...</td>\n",
       "      <td>[-0.01618322244071664, -0.02359793926091037, 0...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        id  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350   \n",
       "2                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  0x1cd5b0   \n",
       "3  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c   \n",
       "4       Still waiting on those supplies Liscus. <LH>  0x2c91a8   \n",
       "\n",
       "                                            unigrams  \\\n",
       "0  [people, who, post, \", add, me, on, #, snapcha...   \n",
       "1  [@, brianklaas, as, we, see, ,, trump, is, dan...   \n",
       "2    [now, issa, is, stalking, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, <, lh, >]   \n",
       "3  [@, riskshow, @, thekevinallison, thx, for, th...   \n",
       "4  [still, waiting, on, those, supplies, liscus, ...   \n",
       "\n",
       "                                              vector       emotion  \n",
       "0  [-0.04398071877257593, 0.005973617156422666, 0...  anticipation  \n",
       "1  [-0.07402511888368858, -0.027571952599088873, ...       sadness  \n",
       "2  [-0.08825499574715533, 0.021097196412366435, 0...          fear  \n",
       "3  [-0.05885132318051363, -0.02492928390044968, 0...           joy  \n",
       "4  [-0.01618322244071664, -0.02359793926091037, 0...  anticipation  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>[confident, of, your, obedience, ,, i, write, ...</td>\n",
       "      <td>[-0.013770567964291183, -0.03728248351083156, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>[\", trust, is, not, the, same, as, faith, ., a...</td>\n",
       "      <td>[0.030071268925277977, -0.036261832915662436, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>[when, do, you, have, enough, ?, when, are, yo...</td>\n",
       "      <td>[0.0605162729192968, -0.0009634782860012661, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>[god, woke, you, up, ,, now, chase, the, day, ...</td>\n",
       "      <td>[0.01819009581685019, 0.001476407360784927, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>[in, these, tough, times, ,, who, do, you, tur...</td>\n",
       "      <td>[0.017859831789070642, 0.09871569549570929, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text        id  \\\n",
       "2   Confident of your obedience, I write to you, k...  0x28b412   \n",
       "4   \"Trust is not the same as faith. A friend is s...  0x2de201   \n",
       "9   When do you have enough ? When are you satisfi...  0x218443   \n",
       "30  God woke you up, now chase the day #GodsPlan #...  0x2939d5   \n",
       "33  In these tough times, who do YOU turn to as yo...  0x26289a   \n",
       "\n",
       "                                             unigrams  \\\n",
       "2   [confident, of, your, obedience, ,, i, write, ...   \n",
       "4   [\", trust, is, not, the, same, as, faith, ., a...   \n",
       "9   [when, do, you, have, enough, ?, when, are, yo...   \n",
       "30  [god, woke, you, up, ,, now, chase, the, day, ...   \n",
       "33  [in, these, tough, times, ,, who, do, you, tur...   \n",
       "\n",
       "                                               vector  \n",
       "2   [-0.013770567964291183, -0.03728248351083156, ...  \n",
       "4   [0.030071268925277977, -0.036261832915662436, ...  \n",
       "9   [0.0605162729192968, -0.0009634782860012661, 0...  \n",
       "30  [0.01819009581685019, 0.001476407360784927, 0....  \n",
       "33  [0.017859831789070642, 0.09871569549570929, 0....  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter useful columns\n",
    "train_tweets_df = train_tweets_df.filter(['id','vector','emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets_df = test_tweets_df.filter(['id','vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop nan value in train and replace nan value with all zero vector in test\n",
    "train_tweets_df = train_tweets_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1455563"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Train models and predict the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Naive Bayes couldn't train negtive values, we use linear SVC and logistic regression to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We couldn't save train_tweets_vec_df.pkl by jupyter since the kernal will be crashed, We save the files in local environment\n",
    "train_tweets_df = pd.read_pickle(\"train_tweets_vec_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare X and Y training data and label\n",
    "X_train = train_tweets_df['vector'][:int(len(train_tweets_df)*3/4)]\n",
    "Y_train = train_tweets_df['emotion'][:int(len(train_tweets_df)*3/4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1091672"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1091672"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#also prepare X testing data\n",
    "X_test = train_tweets_df['vector'][int(len(train_tweets_df)*3/4):]\n",
    "Y_test = train_tweets_df['emotion'][int(len(train_tweets_df)*3/4):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build LinearSVC model\n",
    "LSVC_model = LinearSVC()\n",
    "\n",
    "## training!\n",
    "LSVC_model = LSVC_model.fit(list(X_train), Y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = LSVC_model.predict(list(X_train))\n",
    "y_test_pred = LSVC_model.predict(list(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('LSVC_clf.pickle', 'wb') as f:\n",
    "    pickle.dump(LSVC_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.46\n",
      "testing accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_train = accuracy_score(y_true=Y_train,y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=Y_test,y_pred=y_test_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## build LogisticRegression model\n",
    "LR_model = LogisticRegression()\n",
    "\n",
    "## training!\n",
    "LR_model = LR_model.fit(list(X_train), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('LR_clf.pickle', 'wb') as f:\n",
    "    pickle.dump(LR_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## predict!\n",
    "y_train_pred = LR_model.predict(list(X_train))\n",
    "y_test_pred = LR_model.predict(list(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.47\n",
      "testing accuracy: 0.47\n"
     ]
    }
   ],
   "source": [
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_train = accuracy_score(y_true=Y_train,y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=Y_test,y_pred=y_test_pred)\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) write submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets_df = pd.read_pickle(\"test_tweets_vec_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('LSVC_clf.pickle', 'rb') as f:\n",
    "    LSVC_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_unknown = test_tweets_df['vector']\n",
    "X_unknown = np.array(X_unknown)\n",
    "y_unknown_pred = LSVC_model.predict(list(X_unknown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = {\n",
    "    \"id\":list(test_tweets_df['id']),\n",
    "    \"emotion\":list(y_unknown_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = pd.DataFrame(predict_test)\n",
    "predict_test.to_csv('result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('LR_clf.pickle', 'rb') as f:\n",
    "    LR_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_unknown = test_tweets_df['vector']\n",
    "X_unknown = np.array(X_unknown)\n",
    "y_unknown_pred = LR_model.predict(list(X_unknown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = {\n",
    "    \"id\":list(test_tweets_df['id']),\n",
    "    \"emotion\":list(y_unknown_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = pd.DataFrame(predict_test)\n",
    "predict_test.to_csv('LR_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) result\n",
    "I use 3/4 of training data to build two models, and it almost approached the baseline by logistic regression. I think the reason why they couldn't reach the baseline is because I use a rough way to feature sentences. Averaging all word vectors may not be able to include the meaning of context and the natural relationship between attributes and true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since traditional machine learning with w2v couldn't get high score, I try to use deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For 300 features:\n",
    "#### (1) Transform sentences into 300 features bag-of-words.\n",
    "Though the example in DM19_Lab2_Master uses 500 features, I consider if less features will improve the model or not. Therefore, I set max_features = 300 in first try. Besides, I only use 500k of data to train to avoid from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"train_tweets_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle(\"test_tweets_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "BOW_vectorizer = CountVectorizer(max_features=300, tokenizer=nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_train = 500000\n",
    "# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "BOW_vectorizer.fit(train_df['text'][:num_of_train])\n",
    "\n",
    "# 2. Transform documents to document-term matrix.\n",
    "X_train = BOW_vectorizer.transform(train_df['text'][:num_of_train])\n",
    "y_train = train_df['emotion'][:num_of_train]\n",
    "X_test = BOW_vectorizer.transform(train_df['text'][1400000:])\n",
    "y_test = train_df['emotion'][1400000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Build neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (500000, 300)\n",
      "y_train.shape:  (500000,)\n",
      "X_test.shape:  (55563, 300)\n",
      "y_test.shape:  (55563,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:7]:\n",
      " 0    anticipation\n",
      "1         sadness\n",
      "2            fear\n",
      "3             joy\n",
      "4    anticipation\n",
      "5             joy\n",
      "6         sadness\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (500000,)\n",
      "y_test.shape:  (55563,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:7]:\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (500000, 8)\n",
      "y_test.shape:  (55563, 8)\n"
     ]
    }
   ],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:7]:\\n', y_train[0:7])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:7]:\\n', y_train[0:7])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  300\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                19264     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 23,944\n",
      "Trainable params: 23,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 500000 samples, validate on 55563 samples\n",
      "Epoch 1/25\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "500000/500000 [==============================] - 31s 63us/step - loss: 1.4958 - acc: 0.4539 - val_loss: 1.4674 - val_acc: 0.4599\n",
      "Epoch 2/25\n",
      "500000/500000 [==============================] - 31s 62us/step - loss: 1.4442 - acc: 0.4727 - val_loss: 1.4493 - val_acc: 0.4682\n",
      "Epoch 3/25\n",
      "500000/500000 [==============================] - 32s 65us/step - loss: 1.4295 - acc: 0.4778 - val_loss: 1.4430 - val_acc: 0.4698\n",
      "Epoch 4/25\n",
      "500000/500000 [==============================] - 31s 62us/step - loss: 1.4209 - acc: 0.4812 - val_loss: 1.4416 - val_acc: 0.4715\n",
      "Epoch 5/25\n",
      "500000/500000 [==============================] - 30s 61us/step - loss: 1.4148 - acc: 0.4825 - val_loss: 1.4420 - val_acc: 0.4728\n",
      "Epoch 6/25\n",
      "500000/500000 [==============================] - 29s 58us/step - loss: 1.4103 - acc: 0.4843 - val_loss: 1.4429 - val_acc: 0.4716\n",
      "Epoch 7/25\n",
      "500000/500000 [==============================] - 29s 58us/step - loss: 1.4066 - acc: 0.4860 - val_loss: 1.4424 - val_acc: 0.4717\n",
      "Epoch 8/25\n",
      "500000/500000 [==============================] - 29s 58us/step - loss: 1.4036 - acc: 0.4876 - val_loss: 1.4386 - val_acc: 0.4741\n",
      "Epoch 9/25\n",
      "500000/500000 [==============================] - 28s 56us/step - loss: 1.4012 - acc: 0.4879 - val_loss: 1.4397 - val_acc: 0.4715\n",
      "Epoch 10/25\n",
      "500000/500000 [==============================] - 28s 56us/step - loss: 1.3992 - acc: 0.4888 - val_loss: 1.4408 - val_acc: 0.4727\n",
      "Epoch 11/25\n",
      "500000/500000 [==============================] - 28s 55us/step - loss: 1.3971 - acc: 0.4900 - val_loss: 1.4412 - val_acc: 0.4725\n",
      "Epoch 12/25\n",
      "500000/500000 [==============================] - 30s 60us/step - loss: 1.3955 - acc: 0.4898 - val_loss: 1.4442 - val_acc: 0.4708\n",
      "Epoch 13/25\n",
      "500000/500000 [==============================] - 31s 62us/step - loss: 1.3940 - acc: 0.4909 - val_loss: 1.4474 - val_acc: 0.4690\n",
      "Epoch 14/25\n",
      "500000/500000 [==============================] - 32s 65us/step - loss: 1.3927 - acc: 0.4909 - val_loss: 1.4477 - val_acc: 0.4712\n",
      "Epoch 15/25\n",
      "500000/500000 [==============================] - 33s 66us/step - loss: 1.3915 - acc: 0.4914 - val_loss: 1.4437 - val_acc: 0.4726\n",
      "Epoch 16/25\n",
      "500000/500000 [==============================] - 35s 69us/step - loss: 1.3906 - acc: 0.4922 - val_loss: 1.4472 - val_acc: 0.4714\n",
      "Epoch 17/25\n",
      "500000/500000 [==============================] - 34s 68us/step - loss: 1.3895 - acc: 0.4922 - val_loss: 1.4448 - val_acc: 0.4716\n",
      "Epoch 18/25\n",
      "500000/500000 [==============================] - 35s 70us/step - loss: 1.3887 - acc: 0.4930 - val_loss: 1.4515 - val_acc: 0.4711\n",
      "Epoch 19/25\n",
      "500000/500000 [==============================] - 32s 63us/step - loss: 1.3878 - acc: 0.4934 - val_loss: 1.4502 - val_acc: 0.4718\n",
      "Epoch 20/25\n",
      "500000/500000 [==============================] - 36s 71us/step - loss: 1.3871 - acc: 0.4931 - val_loss: 1.4502 - val_acc: 0.4712\n",
      "Epoch 21/25\n",
      "500000/500000 [==============================] - 39s 77us/step - loss: 1.3864 - acc: 0.4938 - val_loss: 1.4527 - val_acc: 0.4705\n",
      "Epoch 22/25\n",
      "500000/500000 [==============================] - 37s 75us/step - loss: 1.3856 - acc: 0.4947 - val_loss: 1.4550 - val_acc: 0.4715\n",
      "Epoch 23/25\n",
      "500000/500000 [==============================] - 35s 71us/step - loss: 1.3851 - acc: 0.4946 - val_loss: 1.4521 - val_acc: 0.4699\n",
      "Epoch 24/25\n",
      "500000/500000 [==============================] - 34s 69us/step - loss: 1.3846 - acc: 0.4944 - val_loss: 1.4521 - val_acc: 0.4711\n",
      "Epoch 25/25\n",
      "500000/500000 [==============================] - 38s 75us/step - loss: 1.3841 - acc: 0.4940 - val_loss: 1.4528 - val_acc: 0.4714\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')\n",
    "model.save('BOW300_500k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), label_decode(label_encoder,y_test_pred)), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4)  Predict and write submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_unknown = BOW_vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_unknown_pred = model.predict(X_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_unknown_pred = label_decode(label_encoder, y_unknown_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = {\n",
    "    \"id\":list(test_df['id']),\n",
    "    \"emotion\":list(y_unknown_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = pd.DataFrame(predict_test)\n",
    "predict_test.to_csv('BOW300_500k.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first deep learning model couldn't reach the benchmark, so I need to adjust some parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) resume the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The val_acc above seems to glue on about 0.47. I think this may because the number of epochs isn't enough, so I resume the training process in order to improve the val_acc. Below is the second training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model('BOW300_500k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500000 samples, validate on 55563 samples\n",
      "Epoch 1/25\n",
      "500000/500000 [==============================] - 37s 74us/step - loss: 1.3837 - acc: 0.4945 - val_loss: 1.4546 - val_acc: 0.4715\n",
      "Epoch 2/25\n",
      "500000/500000 [==============================] - 34s 68us/step - loss: 1.3833 - acc: 0.4948 - val_loss: 1.4534 - val_acc: 0.4704\n",
      "Epoch 3/25\n",
      "500000/500000 [==============================] - 36s 72us/step - loss: 1.3825 - acc: 0.4952 - val_loss: 1.4551 - val_acc: 0.4708\n",
      "Epoch 4/25\n",
      "500000/500000 [==============================] - 34s 67us/step - loss: 1.3821 - acc: 0.4955 - val_loss: 1.4560 - val_acc: 0.4724\n",
      "Epoch 5/25\n",
      "500000/500000 [==============================] - 34s 68us/step - loss: 1.3814 - acc: 0.4957 - val_loss: 1.4565 - val_acc: 0.4695\n",
      "Epoch 6/25\n",
      "500000/500000 [==============================] - 34s 68us/step - loss: 1.3817 - acc: 0.4958 - val_loss: 1.4565 - val_acc: 0.4713\n",
      "Epoch 7/25\n",
      "500000/500000 [==============================] - 35s 70us/step - loss: 1.3811 - acc: 0.4958 - val_loss: 1.4586 - val_acc: 0.4706\n",
      "Epoch 8/25\n",
      "500000/500000 [==============================] - 36s 72us/step - loss: 1.3806 - acc: 0.4959 - val_loss: 1.4598 - val_acc: 0.4702\n",
      "Epoch 9/25\n",
      "500000/500000 [==============================] - 38s 75us/step - loss: 1.3803 - acc: 0.4963 - val_loss: 1.4632 - val_acc: 0.4686\n",
      "Epoch 10/25\n",
      "500000/500000 [==============================] - 36s 72us/step - loss: 1.3799 - acc: 0.4963 - val_loss: 1.4589 - val_acc: 0.4700\n",
      "Epoch 11/25\n",
      "500000/500000 [==============================] - 34s 68us/step - loss: 1.3800 - acc: 0.4967 - val_loss: 1.4640 - val_acc: 0.4709\n",
      "Epoch 12/25\n",
      "500000/500000 [==============================] - 34s 68us/step - loss: 1.3797 - acc: 0.4966 - val_loss: 1.4600 - val_acc: 0.4691\n",
      "Epoch 13/25\n",
      "500000/500000 [==============================] - 35s 69us/step - loss: 1.3793 - acc: 0.4964 - val_loss: 1.4604 - val_acc: 0.4699\n",
      "Epoch 14/25\n",
      "500000/500000 [==============================] - 35s 71us/step - loss: 1.3789 - acc: 0.4964 - val_loss: 1.4656 - val_acc: 0.4691\n",
      "Epoch 15/25\n",
      "500000/500000 [==============================] - 36s 72us/step - loss: 1.3786 - acc: 0.4968 - val_loss: 1.4589 - val_acc: 0.4693\n",
      "Epoch 16/25\n",
      "500000/500000 [==============================] - 35s 70us/step - loss: 1.3785 - acc: 0.4969 - val_loss: 1.4665 - val_acc: 0.4690\n",
      "Epoch 17/25\n",
      "500000/500000 [==============================] - 39s 78us/step - loss: 1.3784 - acc: 0.4967 - val_loss: 1.4690 - val_acc: 0.4685\n",
      "Epoch 18/25\n",
      "500000/500000 [==============================] - 44s 88us/step - loss: 1.3782 - acc: 0.4964 - val_loss: 1.4616 - val_acc: 0.4714\n",
      "Epoch 19/25\n",
      "500000/500000 [==============================] - 43s 85us/step - loss: 1.3779 - acc: 0.4968 - val_loss: 1.4710 - val_acc: 0.4663\n",
      "Epoch 20/25\n",
      "500000/500000 [==============================] - 43s 86us/step - loss: 1.3774 - acc: 0.4974 - val_loss: 1.4648 - val_acc: 0.4681\n",
      "Epoch 21/25\n",
      "500000/500000 [==============================] - 45s 90us/step - loss: 1.3773 - acc: 0.4977 - val_loss: 1.4697 - val_acc: 0.4678\n",
      "Epoch 22/25\n",
      "500000/500000 [==============================] - 42s 83us/step - loss: 1.3772 - acc: 0.4969 - val_loss: 1.4623 - val_acc: 0.4702\n",
      "Epoch 23/25\n",
      "500000/500000 [==============================] - 49s 98us/step - loss: 1.3771 - acc: 0.4972 - val_loss: 1.4679 - val_acc: 0.4665\n",
      "Epoch 24/25\n",
      "500000/500000 [==============================] - 52s 104us/step - loss: 1.3769 - acc: 0.4974 - val_loss: 1.4613 - val_acc: 0.4695\n",
      "Epoch 25/25\n",
      "500000/500000 [==============================] - 48s 95us/step - loss: 1.3766 - acc: 0.4976 - val_loss: 1.4705 - val_acc: 0.4690\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = new_model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')\n",
    "model.save('second_BOW300_500k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), label_decode(label_encoder,y_test_pred)), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) result\n",
    "More epochs can only improve train accuracy but not val accuracy. This means that more epochs will cause overfitting, so maybe less epochs will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the works above, I also try to build a model with 3/4 of training data and 25 epochs(the work doesn't show in this notebook). But more data couldn't get higher score, too. So I think maybe the problem would be the number of features but not the number of epochs or training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For 500 features:\n",
    "#### use 500 features and more training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, it seems that using more features might be a good choice, too. Thus, I try to use 500 features with 500k data and 25 epochs to train the first 500 features model(the work doesn't show in this notebook). Surprisingly, the score doesn't improve but worse than 300 features with the same number of training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since adjusting the number of features couldn't get higher score, I think more data might be another important fact, too. So, I use 3/4 of training data with 25 epochs to train second model(the work doesn't show in this notebook). This time I finally exceed the benchmark! This result shows that more data can help the model to find true attributes and get a link between these attributes and true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third try is training with more epochs in order to ensure that more epochs doesn't improve the score. I use another 25 epochs to train the model which has already trained by 3/4 data with 25 epochs(the work doesn't show in the no). Indeed, the result isn't higher than the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final try is training with less epochs. Below is the work for building the network and training it with only 9 epochs. It's almost the same as above work. The result is only higher a little bit than 25 epochs. Thus, it seems that the number of epochs doesn't affect a lot in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "BOW_vectorizer = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "num_of_train = int(len(train_df)*3/4)\n",
    "BOW_vectorizer.fit(train_df['text'][:num_of_train])\n",
    "\n",
    "# 2. Transform documents to document-term matrix.\n",
    "X_train = BOW_vectorizer.transform(train_df['text'][:num_of_train])\n",
    "y_train = train_df['emotion'][:num_of_train]\n",
    "X_test = BOW_vectorizer.transform(train_df['text'][num_of_train:])\n",
    "y_test = train_df['emotion'][num_of_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1091672, 500)\n",
      "y_train.shape:  (1091672,)\n",
      "X_test.shape:  (363891, 500)\n",
      "y_test.shape:  (363891,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  500\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1091672 samples, validate on 363891 samples\n",
      "Epoch 1/9\n",
      "1091672/1091672 [==============================] - 105s 96us/step - loss: 1.4227 - acc: 0.4808 - val_loss: 1.3949 - val_acc: 0.4920\n",
      "Epoch 2/9\n",
      "1091672/1091672 [==============================] - 106s 97us/step - loss: 1.3791 - acc: 0.4959 - val_loss: 1.3849 - val_acc: 0.4968\n",
      "Epoch 3/9\n",
      "1091672/1091672 [==============================] - 107s 98us/step - loss: 1.3677 - acc: 0.5005 - val_loss: 1.3784 - val_acc: 0.4978\n",
      "Epoch 4/9\n",
      "1091672/1091672 [==============================] - 106s 97us/step - loss: 1.3611 - acc: 0.5026 - val_loss: 1.3765 - val_acc: 0.4982\n",
      "Epoch 5/9\n",
      "1091672/1091672 [==============================] - 107s 98us/step - loss: 1.3568 - acc: 0.5048 - val_loss: 1.3761 - val_acc: 0.4986\n",
      "Epoch 6/9\n",
      "1091672/1091672 [==============================] - 107s 98us/step - loss: 1.3535 - acc: 0.5058 - val_loss: 1.3745 - val_acc: 0.4996\n",
      "Epoch 7/9\n",
      "1091672/1091672 [==============================] - 106s 97us/step - loss: 1.3510 - acc: 0.5071 - val_loss: 1.3756 - val_acc: 0.4996\n",
      "Epoch 8/9\n",
      "1091672/1091672 [==============================] - 109s 100us/step - loss: 1.3489 - acc: 0.5073 - val_loss: 1.3754 - val_acc: 0.5001\n",
      "Epoch 9/9\n",
      "1091672/1091672 [==============================] - 109s 99us/step - loss: 1.3474 - acc: 0.5087 - val_loss: 1.3741 - val_acc: 0.5001\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 9\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')\n",
    "model.save('BOW500_1350k_e9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), label_decode(label_encoder,y_test_pred)), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_unknown = BOW_vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_unknown_pred = model.predict(X_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\grace\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_unknown_pred = label_decode(label_encoder, y_unknown_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = {\n",
    "    \"id\":list(test_df['id']),\n",
    "    \"emotion\":list(y_unknown_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = pd.DataFrame(predict_test)\n",
    "predict_test.to_csv('1350k_e9_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conclusion\n",
    "From all works above, we can get some conclusions. First, 500 features gets higher training accuracy at the first time than 300 features. That is, more features can include more useful informations. Second, more training data can really get higher score. Third, the number of epochs doesn't affect a lot in this competition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
